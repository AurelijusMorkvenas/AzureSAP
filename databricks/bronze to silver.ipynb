{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4813fdc8-4484-47cf-9eca-f2a284f7a289",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Single table column **transformation****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1c10d6a-afbe-4896-985d-85500c25e83a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List contents of bronze directory to verify our source\n",
    "dbutils.fs.ls('mnt/bronze/SalesLT/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0d04a6d-6778-43b3-ac50-abdb0b3b47d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List contents of silver directory to verify our destination\n",
    "dbutils.fs.ls('mnt/silver/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79f080d1-b8c4-4051-ac67-2f3b46d026f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the Address parquet file from bronze layer into a DataFrame\n",
    "df = spark.read.format('parquet').load('/mnt/bronze/SalesLT/Address/Address.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19af0fa4-6c81-4cce-9185-b4734230af57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the DataFrame to verify the data was loaded correctly\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd49975d-7341-4ad1-aa8d-9596437fa644",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary PySpark functions for date transformation\n",
    "from pyspark.sql.functions import from_utc_timestamp, date_format\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "# Transform ModifiedDate column:\n",
    "# 1. Cast the column to TimestampType\n",
    "# 2. Convert from UTC timezone\n",
    "# 3. Format the date as YYYY-MM-DD\n",
    "df = df.withColumn('ModifiedDate', date_format(from_utc_timestamp(df['ModifiedDate'].cast(TimestampType()), 'UTC'), 'yyyy-MM-dd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "775d57a1-5ca1-4fd1-bdb7-de0c63c12db4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the DataFrame to verify the date transformation\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3124fbf-8197-40f1-bd50-7e0f08790763",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Date transformation for all the tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e63807c0-b782-4102-b5c5-e8b2d91df5fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize empty list to store table names\n",
    "table_name = []\n",
    "\n",
    "# Get all table names from the bronze layer\n",
    "# The split('/')[0] removes the trailing slash from directory names\n",
    "for i in dbutils.fs.ls('mnt/bronze/SalesLT/'):\n",
    "    table_name.append(i.name.split('/')[0])\n",
    "\n",
    "# Display list of tables that will be processed\n",
    "table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5f14b31-1e31-4b30-a071-67bc9673a083",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required functions (if running in a new session)\n",
    "from pyspark.sql.functions import from_utc_timestamp, date_format\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "# Process each table in the bronze layer\n",
    "for i in table_name:\n",
    "    # Load the parquet file into a DataFrame\n",
    "    path = '/mnt/bronze/SalesLT/' + i + '/' + i + '.parquet'\n",
    "\n",
    "    # Read the parquet file\n",
    "    df = spark.read.format('parquet').load(path)\n",
    "\n",
    "    # Get all column names in the current table\n",
    "    column = df.columns\n",
    "\n",
    "    # Look for date columns and transform them\n",
    "    for col in column:\n",
    "\n",
    "        # Check if the column name contains 'Date' or 'date'\n",
    "        if \"Date\" in col or \"date\" in col:\n",
    "\n",
    "            # Transform the date column:\n",
    "            # 1. Cast to TimestampType\n",
    "            # 2. Convert from UTC\n",
    "            # 3. Format as YYYY-MM-DD\n",
    "            df = df.withColumn(col, date_format(from_utc_timestamp(df[col].cast(TimestampType()), \"UTC\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "    # Construct the output path in the silver layer\n",
    "    output_path = '/mnt/silver/SalesLT/' + i + '/'\n",
    "\n",
    "    # Write the transformed DataFrame to silver layer:\n",
    "    # - Using delta format for better data management\n",
    "    # - Overwrite mode replaces existing data\n",
    "    df.write.format('delta').mode('overwrite').save(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27aa1a6d-adc7-484d-a8d1-9980b0f40e5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the final transformed DataFrame (last table processed)\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze to silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
